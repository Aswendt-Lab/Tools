# ğŸ§  DataLad ZIP Preparation Workflow  
### Folder-Driven, Resumable, Manual-Control First

This repository provides a **safe and resumable workflow** for preparing large ZIP archives in a **DataLad / git-annex dataset** before manual inspection, extraction, and later committing.

The core idea is simple:

> ğŸ“ **Folders are truth. ZIPs are not and should be avoided/not used in DataLad datasets**

---

## ğŸš¦ Core Behavior (at a Glance)

âœ… Iterate **one ZIP at a time**  
âœ… Resume safely after interruption  
âœ… Skip anything already completed  
âŒ No unzip  
âŒ No save / push / drop  
ğŸ§‘â€ğŸ”¬ Manual inspection encouraged  

---

## ğŸ”‘ Key Rule (Very Important)

### ğŸŸ¢ Continue **only if the folder is missing**

| Condition | Action |
|---------|--------|
| ğŸ“ Folder exists | â­ï¸ Skip completely |
| ğŸ“ Folder missing | â–¶ï¸ Process ZIP |

â¡ï¸ ZIP state (present, unlocked, partial, failed) is **ignored**

This guarantees:
- no accidental overwrites
- no false positives from partial work
- safe reruns across commits and machines

---

## ğŸ”„ What the Script Does

For each `*.zip` file:

1ï¸âƒ£ Derives the target folder name  
2ï¸âƒ£ Checks whether that folder already exists  
3ï¸âƒ£ If missing:
- ğŸ“¥ `datalad get`
- ğŸ”“ `datalad unlock`
4ï¸âƒ£ Moves on to the next ZIP  

ğŸ›‘ **After every N ZIPs (default: 5):**
- pauses
- waits for user input
- allows manual inspection

---

## ğŸ§‘â€ğŸ”¬ What the Script Intentionally Does *NOT* Do

âŒ No unzip (ZIPs may be huge / slow / fragile)  
âŒ No `datalad save`  
âŒ No `datalad push`  
âŒ No `datalad drop`  

â¡ï¸ All destructive or irreversible actions are **user-controlled**

---

## ğŸ“¦ Why Folder-Driven Logic?

ZIP-based logic is unreliable because ZIPs can be:

âš ï¸ partially downloaded  
âš ï¸ previously unlocked  
âš ï¸ present but never validated  
âš ï¸ corrupted but still large  

Folders, however, mean:

âœ… data extracted  
âœ… structure verified  
âœ… work completed  

If a folder exists, the script assumes:
> â€œThis dataset entry is done.â€

---

## â¸ï¸ Batch Processing & Pausing

- Default batch size: **5**
- After each batch:
  - â¸ï¸ script pauses
  - ğŸ‘€ user inspects ZIPs
  - ğŸ§ª optional manual unzip & validation
  - â–¶ï¸ user resumes explicitly

Perfect for:
- slow networks
- large archives
- long annex transfers

---

## ğŸ” Safe to Re-Run (Any Time)

You can re-run the script:

- after crashes
- after switching commits
- after partial downloads
- after manual cleanup

Already existing folders are **never touched again**.

---

## ğŸ§­ Typical Workflow

ğŸ”¹ Run script  
ğŸ”¹ Wait for pause  
ğŸ”¹ Inspect ZIPs  
ğŸ”¹ Unzip manually  
ğŸ”¹ Validate folder size / content  
ğŸ”¹ Later: run a **separate** save/push/drop script  

---

## ğŸ›¡ï¸ Safety Guarantees

âœ”ï¸ No automatic extraction  
âœ”ï¸ No automatic commits  
âœ”ï¸ No data deletion  
âœ”ï¸ No forced retries  
âœ”ï¸ Errors logged, loop continues  

---

## ğŸ¯ Intended Use Case

- Large MRI / neuroimaging datasets  
- git-annex backed storage  
- Long-running or unstable transfers  
- Human-in-the-loop QA before commits  

---

## ğŸ“œ License

Use freely.  
No warranty.  
You break it, you keep both pieces ğŸ˜‰
